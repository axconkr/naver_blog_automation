"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬

íŠ¹ì • ë„¤ì´ë²„ ë¸”ë¡œê·¸ì˜ í¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List, Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import google.generativeai as genai


class NaverBlogCrawler:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬"""

    def __init__(self, headless: bool = False):
        """
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‹¤í–‰ ì—¬ë¶€
        """
        self.headless = headless
        self.driver = None

    def init_driver(self):
        """WebDriver ì´ˆê¸°í™”"""
        options = Options()
        if self.headless:
            options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')

        self.driver = webdriver.Chrome(options=options)
        print("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ")

    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("âœ… WebDriver ì¢…ë£Œ")

    def get_blog_post_urls(self, blog_id: str, max_posts: int = 10) -> List[str]:
        """
        ë¸”ë¡œê·¸ì˜ í¬ìŠ¤íŠ¸ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°

        Args:
            blog_id: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ID (ì˜ˆ: chikkqueen)
            max_posts: ìˆ˜ì§‘í•  ìµœëŒ€ í¬ìŠ¤íŠ¸ ê°œìˆ˜

        Returns:
            í¬ìŠ¤íŠ¸ URL ëª©ë¡
        """
        if not self.driver:
            self.init_driver()

        blog_url = f"https://blog.naver.com/{blog_id}"
        self.driver.get(blog_url)
        time.sleep(3)

        post_urls = []

        try:
            # iframeìœ¼ë¡œ ì „í™˜
            self.driver.switch_to.frame("mainFrame")

            # í¬ìŠ¤íŠ¸ ë§í¬ ì°¾ê¸°
            post_links = self.driver.find_elements(By.CSS_SELECTOR, "a.link_title")

            for link in post_links[:max_posts]:
                url = link.get_attribute('href')
                if url:
                    post_urls.append(url)

            self.driver.switch_to.default_content()

        except Exception as e:
            print(f"âš ï¸ í¬ìŠ¤íŠ¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.driver.switch_to.default_content()

        print(f"ğŸ“ {len(post_urls)}ê°œì˜ í¬ìŠ¤íŠ¸ URL ìˆ˜ì§‘ë¨")
        return post_urls

    def extract_post_content(self, post_url: str) -> Dict:
        """
        í¬ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ

        Args:
            post_url: í¬ìŠ¤íŠ¸ URL

        Returns:
            í¬ìŠ¤íŠ¸ ë°ì´í„° (ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€ ë“±)
        """
        if not self.driver:
            self.init_driver()

        self.driver.get(post_url)
        time.sleep(2)

        post_data = {
            'url': post_url,
            'title': '',
            'content': '',
            'raw_html': '',
            'images': [],
            'structure': {},
            'crawled_at': datetime.now().isoformat()
        }

        try:
            # iframeìœ¼ë¡œ ì „í™˜
            self.driver.switch_to.frame("mainFrame")

            # ì œëª©
            try:
                title_elem = self.driver.find_element(By.CSS_SELECTOR, ".se-title-text")
                post_data['title'] = title_elem.text
            except NoSuchElementException:
                pass

            # ë³¸ë¬¸ ë‚´ìš© (í…ìŠ¤íŠ¸)
            try:
                content_elem = self.driver.find_element(By.CSS_SELECTOR, ".se-main-container")
                post_data['content'] = content_elem.text
                post_data['raw_html'] = content_elem.get_attribute('innerHTML')
            except NoSuchElementException:
                pass

            # ì´ë¯¸ì§€ URL ìˆ˜ì§‘
            try:
                images = self.driver.find_elements(By.CSS_SELECTOR, "img.se-image-resource")
                for img in images:
                    img_url = img.get_attribute('src')
                    if img_url:
                        post_data['images'].append(img_url)
            except NoSuchElementException:
                pass

            # êµ¬ì¡° ë¶„ì„ (ë¬¸ë‹¨, ì œëª© ë“±)
            post_data['structure'] = self._analyze_structure()

            self.driver.switch_to.default_content()

        except Exception as e:
            print(f"âš ï¸ í¬ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            self.driver.switch_to.default_content()

        return post_data

    def _analyze_structure(self) -> Dict:
        """í¬ìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„ (iframe ë‚´ë¶€ì—ì„œ í˜¸ì¶œ)"""
        structure = {
            'has_heading': False,
            'has_subheading': False,
            'has_bold': False,
            'has_italic': False,
            'has_list': False,
            'has_link': False,
            'has_quote': False,
            'paragraph_count': 0,
            'image_count': 0,
            'text_align': 'left'  # ê¸°ë³¸ê°’
        }

        try:
            # ì œëª© íƒœê·¸
            if self.driver.find_elements(By.CSS_SELECTOR, ".se-text-heading"):
                structure['has_heading'] = True

            # ë¶€ì œëª©
            if self.driver.find_elements(By.CSS_SELECTOR, ".se-text-subheading"):
                structure['has_subheading'] = True

            # ë³¼ë“œ
            if self.driver.find_elements(By.CSS_SELECTOR, "strong, b"):
                structure['has_bold'] = True

            # ì´íƒ¤ë¦­
            if self.driver.find_elements(By.CSS_SELECTOR, "em, i"):
                structure['has_italic'] = True

            # ë¦¬ìŠ¤íŠ¸
            if self.driver.find_elements(By.CSS_SELECTOR, "ul, ol"):
                structure['has_list'] = True

            # ë§í¬
            if self.driver.find_elements(By.CSS_SELECTOR, "a"):
                structure['has_link'] = True

            # ì¸ìš©êµ¬
            if self.driver.find_elements(By.CSS_SELECTOR, "blockquote"):
                structure['has_quote'] = True

            # ë¬¸ë‹¨ ìˆ˜
            paragraphs = self.driver.find_elements(By.CSS_SELECTOR, ".se-text-paragraph")
            structure['paragraph_count'] = len(paragraphs)

            # ì´ë¯¸ì§€ ìˆ˜
            images = self.driver.find_elements(By.CSS_SELECTOR, "img.se-image-resource")
            structure['image_count'] = len(images)

            # í…ìŠ¤íŠ¸ ì •ë ¬ (ì²« ë²ˆì§¸ ë¬¸ë‹¨ ê¸°ì¤€)
            if paragraphs:
                text_align = paragraphs[0].value_of_css_property('text-align')
                structure['text_align'] = text_align

        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

        return structure

    def crawl_blog(self, blog_id: str, max_posts: int = 5) -> List[Dict]:
        """
        ë¸”ë¡œê·¸ ì „ì²´ í¬ë¡¤ë§

        Args:
            blog_id: ë¸”ë¡œê·¸ ID
            max_posts: ìˆ˜ì§‘í•  ìµœëŒ€ í¬ìŠ¤íŠ¸ ê°œìˆ˜

        Returns:
            í¬ìŠ¤íŠ¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ•·ï¸ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘: {blog_id}")
        print("="*60)

        # í¬ìŠ¤íŠ¸ URL ìˆ˜ì§‘
        post_urls = self.get_blog_post_urls(blog_id, max_posts)

        if not post_urls:
            print("âŒ ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []

        # ê° í¬ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        posts = []
        for i, url in enumerate(post_urls, 1):
            print(f"\n[{i}/{len(post_urls)}] í¬ë¡¤ë§: {url}")
            post_data = self.extract_post_content(url)

            if post_data['title']:
                print(f"  âœ… ì œëª©: {post_data['title'][:50]}...")
                print(f"     ë³¸ë¬¸: {len(post_data['content'])}ì")
                print(f"     ì´ë¯¸ì§€: {len(post_data['images'])}ê°œ")
                posts.append(post_data)
            else:
                print("  âš ï¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")

            time.sleep(2)  # ìš”ì²­ ê°„ê²©

        return posts

    def save_to_json(self, blog_id: str, posts: List[Dict], filename: str = None):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        if not filename:
            filename = f"blog_crawl_{blog_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        data = {
            'blog_id': blog_id,
            'crawled_at': datetime.now().isoformat(),
            'post_count': len(posts),
            'posts': posts
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename


class BlogStyleAnalyzer:
    """ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ë¶„ì„ê¸°"""

    def __init__(self, gemini_api_key: str):
        """
        Args:
            gemini_api_key: Gemini API í‚¤
        """
        self.gemini_api_key = gemini_api_key
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')

    def analyze_blog_style(self, posts: List[Dict]) -> Dict:
        """
        ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë“¤ì„ ë¶„ì„í•´ì„œ ìŠ¤íƒ€ì¼ ì¶”ì¶œ

        Args:
            posts: í¬ìŠ¤íŠ¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ìŠ¤íƒ€ì¼ ë¶„ì„ ê²°ê³¼
        """
        if not posts:
            return {}

        print("\nğŸ” ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ ë¶„ì„ ì¤‘...")

        # ì „ì²´ í¬ìŠ¤íŠ¸ ì •ë³´ ìš”ì•½
        summary = {
            'total_posts': len(posts),
            'avg_content_length': sum(len(p['content']) for p in posts) / len(posts),
            'avg_image_count': sum(len(p['images']) for p in posts) / len(posts),
            'structure_patterns': self._aggregate_structures([p['structure'] for p in posts])
        }

        # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì²˜ìŒ 3ê°œ í¬ìŠ¤íŠ¸)
        sample_texts = []
        for post in posts[:3]:
            sample_texts.append({
                'title': post['title'],
                'content': post['content'][:500]  # ì²˜ìŒ 500ìë§Œ
            })

        # Geminië¡œ ìŠ¤íƒ€ì¼ ë¶„ì„
        ai_analysis = self._analyze_with_gemini(sample_texts, summary)

        # ìµœì¢… ê²°ê³¼ í†µí•©
        style_profile = {
            'blog_summary': summary,
            'ai_analysis': ai_analysis,
            'analyzed_at': datetime.now().isoformat()
        }

        return style_profile

    def _aggregate_structures(self, structures: List[Dict]) -> Dict:
        """êµ¬ì¡° íŒ¨í„´ ì§‘ê³„"""
        if not structures:
            return {}

        total = len(structures)
        aggregated = {
            'heading_usage': sum(s.get('has_heading', False) for s in structures) / total * 100,
            'bold_usage': sum(s.get('has_bold', False) for s in structures) / total * 100,
            'list_usage': sum(s.get('has_list', False) for s in structures) / total * 100,
            'avg_paragraph_count': sum(s.get('paragraph_count', 0) for s in structures) / total,
            'avg_image_count': sum(s.get('image_count', 0) for s in structures) / total,
            'common_text_align': self._most_common([s.get('text_align', 'left') for s in structures])
        }

        return aggregated

    def _most_common(self, items: List) -> str:
        """ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ í•­ëª©"""
        if not items:
            return ''
        return max(set(items), key=items.count)

    def _analyze_with_gemini(self, sample_texts: List[Dict], summary: Dict) -> Dict:
        """Geminië¥¼ ì‚¬ìš©í•œ ìŠ¤íƒ€ì¼ ë¶„ì„"""
        prompt = f"""ë‹¤ìŒì€ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒ˜í”Œì…ë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ì˜ ì‘ì„± ìŠ¤íƒ€ì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

# í¬ìŠ¤íŠ¸ ìƒ˜í”Œ
{json.dumps(sample_texts, ensure_ascii=False, indent=2)}

# êµ¬ì¡° í†µê³„
- í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {summary['avg_content_length']:.0f}ì
- í‰ê·  ì´ë¯¸ì§€ ìˆ˜: {summary['avg_image_count']:.1f}ê°œ
- ì œëª© ì‚¬ìš©ë¥ : {summary['structure_patterns'].get('heading_usage', 0):.0f}%
- ë³¼ë“œ ì‚¬ìš©ë¥ : {summary['structure_patterns'].get('bold_usage', 0):.0f}%

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{{
  "tone": "ì–´íˆ¬ ë° í†¤ (ì˜ˆ: ì¹œê·¼í•œ, ì „ë¬¸ì ì¸, ìºì£¼ì–¼í•œ, ê²©ì‹ìˆëŠ”)",
  "sentence_style": "ë¬¸ì¥ ìŠ¤íƒ€ì¼ (ì˜ˆ: ì§§ê³  ê°„ê²°, ê¸¸ê³  ìƒì„¸, ëŒ€í™”ì²´, ì„¤ëª…ì²´)",
  "paragraph_style": "ë¬¸ë‹¨ êµ¬ì„± (ì˜ˆ: ì§§ì€ ë¬¸ë‹¨ ì„ í˜¸, ê¸´ ë¬¸ë‹¨, í•œ ë¬¸ì¥ì”© ì¤„ë°”ê¿ˆ)",
  "emoji_usage": "ì´ëª¨ì§€ ì‚¬ìš© íŒ¨í„´ (ì˜ˆ: ìì£¼ ì‚¬ìš©, ê°€ë” ì‚¬ìš©, ì‚¬ìš© ì•ˆí•¨)",
  "formatting_preferences": ["ìì£¼ ì‚¬ìš©í•˜ëŠ” í¬ë§·íŒ… ìŠ¤íƒ€ì¼ë“¤"],
  "opening_style": "ê¸€ ì‹œì‘ ìŠ¤íƒ€ì¼ (ì˜ˆ: ì¸ì‚¬ë§, ë°”ë¡œ ë³¸ë¡ , ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘)",
  "closing_style": "ê¸€ ë§ˆë¬´ë¦¬ ìŠ¤íƒ€ì¼ (ì˜ˆ: ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬, ê°ì‚¬ì¸ì‚¬, í•´ì‹œíƒœê·¸)",
  "special_patterns": ["íŠ¹ì´í•œ íŒ¨í„´ì´ë‚˜ ìŠµê´€"],
  "keyword_patterns": ["ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ë‚˜ í‘œí˜„"],
  "image_placement": "ì´ë¯¸ì§€ ë°°ì¹˜ íŒ¨í„´ (ì˜ˆ: ì¤‘ê°„ì¤‘ê°„ ì‚½ì…, ë¬¸ë‹¨ë§ˆë‹¤, ìƒë‹¨ ì§‘ì¤‘)",
  "overall_impression": "ì „ì²´ì ì¸ ëŠë‚Œ í•œ ë¬¸ì¥ ìš”ì•½"
}}

ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”."""

        try:
            response = self.model.generate_content(prompt)
            result_text = response.text.strip()

            # JSON íŒŒì‹±
            if result_text.startswith('```json'):
                result_text = result_text[7:]
            if result_text.startswith('```'):
                result_text = result_text[3:]
            if result_text.endswith('```'):
                result_text = result_text[:-3]

            return json.loads(result_text.strip())

        except Exception as e:
            print(f"âš ï¸ Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'tone': 'unknown',
                'sentence_style': 'unknown',
                'overall_impression': 'ë¶„ì„ ì‹¤íŒ¨'
            }


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    from dotenv import load_dotenv
    load_dotenv()

    # ë¸”ë¡œê·¸ ID ëª©ë¡
    blog_ids = [
        'chikkqueen',
        'chick_king',
        'chick_queen',
        '1234ssem'
    ]

    crawler = NaverBlogCrawler(headless=False)
    analyzer = BlogStyleAnalyzer(os.getenv('GEMINI_API_KEY'))

    try:
        for blog_id in blog_ids:
            # í¬ë¡¤ë§
            posts = crawler.crawl_blog(blog_id, max_posts=5)

            if posts:
                # JSON ì €ì¥
                json_file = crawler.save_to_json(blog_id, posts)

                # ìŠ¤íƒ€ì¼ ë¶„ì„
                style_profile = analyzer.analyze_blog_style(posts)

                # ìŠ¤íƒ€ì¼ í”„ë¡œí•„ ì €ì¥
                style_file = f"blog_style_{blog_id}.json"
                with open(style_file, 'w', encoding='utf-8') as f:
                    json.dump(style_profile, f, ensure_ascii=False, indent=2)

                print(f"âœ… ìŠ¤íƒ€ì¼ í”„ë¡œí•„ ì €ì¥: {style_file}")
                print(f"\nì „ì²´ ì¸ìƒ: {style_profile['ai_analysis'].get('overall_impression', 'N/A')}")

            print("\n" + "="*60 + "\n")

    finally:
        crawler.close_driver()


if __name__ == "__main__":
    main()
